{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T00:05:10.632729Z","iopub.execute_input":"2022-05-03T00:05:10.633371Z","iopub.status.idle":"2022-05-03T00:05:10.659536Z","shell.execute_reply.started":"2022-05-03T00:05:10.633267Z","shell.execute_reply":"2022-05-03T00:05:10.658846Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import imdb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:05:11.634579Z","iopub.execute_input":"2022-05-03T00:05:11.635185Z","iopub.status.idle":"2022-05-03T00:05:19.059007Z","shell.execute_reply.started":"2022-05-03T00:05:11.635148Z","shell.execute_reply":"2022-05-03T00:05:19.057992Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf = df.drop(['id', 'keyword', 'location'], axis = 1)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:07:07.997331Z","iopub.execute_input":"2022-05-03T00:07:07.998202Z","iopub.status.idle":"2022-05-03T00:07:08.075973Z","shell.execute_reply.started":"2022-05-03T00:07:07.998154Z","shell.execute_reply":"2022-05-03T00:07:08.075412Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = df.reset_index(drop=True)\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    text = text.replace('x', '')\n#    text = re.sub(r'\\W+', '', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text\n\ndf['text'] = df['text'].apply(clean_text)\ndf['text'] = df['text'].str.replace('\\d+', '')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:18:29.845515Z","iopub.execute_input":"2022-05-03T00:18:29.846518Z","iopub.status.idle":"2022-05-03T00:18:29.916147Z","shell.execute_reply.started":"2022-05-03T00:18:29.846463Z","shell.execute_reply":"2022-05-03T00:18:29.915195Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 40000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 200\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:52:01.446929Z","iopub.execute_input":"2022-05-03T00:52:01.447861Z","iopub.status.idle":"2022-05-03T00:52:01.626400Z","shell.execute_reply.started":"2022-05-03T00:52:01.447813Z","shell.execute_reply":"2022-05-03T00:52:01.625525Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df['text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\nprint(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:52:02.055545Z","iopub.execute_input":"2022-05-03T00:52:02.056356Z","iopub.status.idle":"2022-05-03T00:52:02.185476Z","shell.execute_reply.started":"2022-05-03T00:52:02.056312Z","shell.execute_reply":"2022-05-03T00:52:02.184505Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"y = np.array(df['target'])\ny","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:52:02.682452Z","iopub.execute_input":"2022-05-03T00:52:02.682710Z","iopub.status.idle":"2022-05-03T00:52:02.690606Z","shell.execute_reply.started":"2022-05-03T00:52:02.682682Z","shell.execute_reply":"2022-05-03T00:52:02.689812Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T00:52:03.861332Z","iopub.execute_input":"2022-05-03T00:52:03.862094Z","iopub.status.idle":"2022-05-03T00:52:03.876876Z","shell.execute_reply.started":"2022-05-03T00:52:03.862049Z","shell.execute_reply":"2022-05-03T00:52:03.875962Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"words = 40000\nmax_length=200\n\nword_size=words\nword_size\n\nembed_size=100\n\n\"\"\"### Building a Recurrent Neural Network\"\"\"\n\nimdb_model=tf.keras.Sequential()\n\n# Embedding Layer\nimdb_model.add(tf.keras.layers.Embedding(word_size, embed_size, input_shape=(X_train.shape[1],)))\n\n# LSTM Layer\nimdb_model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))\n\n# Output Layer\nimdb_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\nimdb_model.summary()\n\n\"\"\"#### Compiling the model\"\"\"\n\nimdb_model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['accuracy'])\n\n\"\"\"#### Training the model\"\"\"\n\nimdb_model.fit(X_train, Y_train, epochs=10, batch_size=128)\n\ntest_loss, test_acurracy = imdb_model.evaluate(X_test, Y_test)\n\nprint(\"Test accuracy: {}\".format(test_acurracy))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T01:15:36.607141Z","iopub.execute_input":"2022-05-03T01:15:36.607412Z","iopub.status.idle":"2022-05-03T01:18:39.385665Z","shell.execute_reply.started":"2022-05-03T01:15:36.607384Z","shell.execute_reply":"2022-05-03T01:18:39.385117Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}